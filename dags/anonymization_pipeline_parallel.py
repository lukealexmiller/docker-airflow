import os
import shutil
import logging

import airflow
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime.now(),
    'email': ['lm@cartwatch.de'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=1),
}

# Define DAG scheduling
dag = DAG(
        dag_id='anonymization_pipeline_parallel',
        default_args=default_args,
        schedule_interval='@once')

"""Python functions and Bash commands defined below are called by the task 
   operators forming the nodes on the DAG.
"""

def list_files(*args, **kwargs):
    """Return list of paths to files in a specified directory. Note that the 
       base directory path is omitted from the list of returned paths.
    """
    source_dir = kwargs["source_location"]
    # Create flattened list of all files in source dir and subdirectories and remove source_dir #.replace(source_dir+"/","")
    source_dir_files = [val for sublist in [[os.path.join(i[0], j) for j in i[2]] for i in os.walk(source_dir)] for val in sublist]
    return source_dir_files

def diff_directories(*args, **kwargs):
    """Compare two directories which are passed by the task and return a list
       of file paths which have a specific extension and are unique to the 
       source directory.
    """
    source_dir = kwargs["source_location"]
    logging.info('Accessing source directory at: {}'.format(source_dir))
    target_dir = kwargs["target_location"]
    logging.info('Accessing target directory at: {}'.format(target_dir))

    valid_extension = kwargs["extension"]
    ignore_dirs = kwargs["ignore_location"]

    # Get flattened list of all files in source and target dirs and their subdirectories
    source_dir_files = list_files(**{'source_location':source_dir})
    source_dir_files = [file.replace(source_dir+"/","") for file in source_dir_files]
    [logging.info("{}".format(file)) for file in source_dir_files] #TODO: Remove
    
    target_dir_files = list_files(**{'source_location':target_dir})
    target_dir_files = [file.replace(target_dir+"/","") for file in target_dir_files]
    [logging.info("{}".format(file)) for file in target_dir_files] # TODO: Remove

    # Filter for files which are unique to the source dir
    logging.info('Filtering for files unique to source directory')
    source_dir_files_unique = list(set(source_dir_files).difference(target_dir_files))
    logging.info('Number of unique files: {}'.format(len(source_dir_files_unique)))

    # Filter for files with correct extension
    logging.info('Filtering for files with extension: {}'.format(valid_extension))
    source_dir_files_unique = [file for file in source_dir_files_unique if file.endswith(valid_extension)]
    logging.info('Number of passing files: {}'.format(len(source_dir_files_unique)))

    # Filter out files in the ignore dirs
    logging.info('Filtering out files in ignored directory: {}'.format(','.join(ignore_dirs)))
    source_dir_files_unique = [file for ignore_dir in ignore_dirs for file in source_dir_files_unique if ignore_dir not in file]
    logging.info('Number of passing files: {}'.format(len(source_dir_files_unique)))
    
    return source_dir_files_unique

def copy_and_delete_files(source_dir, target_dir, file_path):
    """Copy file at source_dir/file_path to target_dir/file_path. This 
       function is used instead of shutil.move() because we cannot copy the
       metadata due to permissions.
    """
    source_file = os.path.join(source_dir, file_path)
    target_file = os.path.join(target_dir, file_path)
    if not os.path.exists(target_file):
        if not os.path.exists(os.path.dirname(target_file)):
            os.makedirs(os.path.dirname(target_file))
        shutil.copyfile(source_file, target_file)
        os.remove(source_file)
        if len(os.listdir(source_dir)) == 0:
            os.rmdir(source_dir) #TODO: THIS COULD CAUSE PROBLEMS

def move_files(*args, **kwargs):
    """Move files from source directory to target directory. The list of files
       which are to be moved are generated by diff_directories and are pulled 
       in to the function as a list using XCOM from a user-defined task_id.
       TODO: Empty directories are not deleted.
    """
    source_dir = kwargs["source_location"]
    logging.info('Accessing source directory at: {}'.format(source_dir))
    target_dir = kwargs["target_location"]
    logging.info('Accessing target directory at: {}'.format(target_dir))
    task_id = kwargs["task_id"]

    ti = kwargs['task_instance']
    files_to_move = ti.xcom_pull(task_ids=task_id)
    logging.info("Number of files to move: {}".format(len(files_to_move)))

    # Move files from source dir which do not currently exist on target dir
    [copy_and_delete_files(source_dir,target_dir,file) for file in files_to_move]
    logging.info("File transfer complete")


def list_unanon_files(*args, **kwargs):
    """Decode video file, extract frames and write to jpeg on server.
    TODO: Pull the jpeg counting into this?
    """
    source_dir = kwargs["source_location"]
    target_dir = kwargs["target_location"]
    n_frame_threshold = kwargs['n_frame_threshold']
    unanon_file = kwargs['unanon_file']

    raw_data_server_files = []
    
    ti = kwargs['task_instance']
    #raw_data_server_files = ti.xcom_pull(task_ids='list_jpeg_on_server_{:d}'.format(num_id))
    #unanon_files = ti.xcom_pull(task_ids='sense_unanon_data_syno')

    #[logging.info("{}".format(file)) for file in unanon_files]

    source_file_path = os.path.join(source_dir,unanon_file) #TODO: How to get a different file? unanon_files[0]
    target_file_path = os.path.join(target_dir,unanon_file)
    target_file_path = os.path.splitext(target_file_path)[0]

    logging.info("Number of files currently on the server: \
        {}".format(len(raw_data_server_files)))

    if len(raw_data_server_files) > n_frame_threshold:
        logging.info("Warning! Number of files currently on the server \
            exceeds threshold of {}".format(n_frame_threshold))
        return 

    logging.info("Converting video file: {}".format(source_file_path))
    logging.info("Extracting frames to: {}".format(target_file_path))
    if not os.path.exists(target_file_path):
        logging.info('Creating directory')
        os.makedirs(target_file_path)

    ti.xcom_push(key='source_file_path', value=source_file_path)
    ti.xcom_push(key='target_file_path', value=target_file_path)

    return


split_video_to_frames_write_to_server = """
    ffmpeg \
    -i {{task_instance.xcom_pull(task_ids='list_unanon_files_{params.num_id}', key='source_file_path')}} \
    -f image2 \
    -s {{params.width}}x{{params.height}} \
    {{task_instance.xcom_pull(task_ids='list_unanon_files_{params.num_id}', key='target_file_path')}}/%06d.{{params.format}}
"""


def create_anon_dir(*args, **kwargs): #TODO: MAKE THIS CREATE_DIR called with ti
    ti = kwargs['task_instance']
    source_file_path = ti.xcom_pull(task_ids='list_unanon_files', key='source_file_path')
    target_file_path = source_file_path.replace('raw_data','anon_data')
    target_dir = os.path.dirname(target_file_path)
    if not os.path.exists(target_dir):
        logging.info("Creating directory: {}".format(target_dir))
        os.makedirs(target_dir)
    return target_file_path


tensorflow_face_detect = """
python /usr/local/airflow/dags/inference.py \
  --gpu {{params.gpu}} \
  --weights {{params.weights}} \
  --cfg {{params.cfg}} \
  --network {{params.network}} \
  --image_dir {{task_instance.xcom_pull(task_ids='list_unanon_files', key='target_file_path')}} \
  --num_classes {{params.num_classes}}
"""

apply_face_blur = """
python /usr/local/airflow/dags/face_blur.py \
  --image_dir {{task_instance.xcom_pull(task_ids='list_unanon_files', key='target_file_path')}} \
  --threshold {{params.threshold}} \
  --class_id {{params.class_id}} \
  --kernel {{params.kernel}} \
  --sigma {{params.sigma}} \
  --compression {{params.compression}}
"""

combine_frames_to_video_write_to_syno = """
{% set source_path = task_instance.xcom_pull(task_ids='list_unanon_files', key='target_file_path') %}
{% set target_path = task_instance.xcom_pull(task_ids='create_anon_dir') %}
mv "{{source_path}}/detections.pkl" \
   "$(dirname $"{{target_path}}")"/"$(basename $"{{target_path}}" .mp4)_detections.pkl"
   
ffmpeg \
    -f image2 \
    -pattern_type glob \
    -i \'{{source_path}}/*.{{params.format}}\' \
    -s {{params.width}}x{{params.height}} \
    -framerate {{params.fps}} \
    {{target_path}}
rm {{source_path}}/*.{{params.format}}
"""


# Define tasks nodes in DAG
t_check_new_data_available = PythonOperator(
        task_id='sense_raw_data_hdd',
        python_callable=diff_directories,
        op_kwargs={'source_location': '/usr/local/airflow/hdd/raw_data',
                   'target_location': '/usr/local/airflow/syno/raw_data',
                   'extension': '.mp4',
                   'ignore_location' : ['#recycle','name','lost+found']},
        dag=dag)

t_move_new_data_to_syno = PythonOperator(
        task_id='move_new_data_to_syno',
        python_callable=move_files,
        provide_context=True,
        op_kwargs={'source_location': '/usr/local/airflow/hdd/raw_data',
                   'target_location': '/usr/local/airflow/syno/raw_data',
                   'task_id': 'sense_raw_data_hdd'},
        dag=dag)

"""
t_list_jpeg_on_server = PythonOperator(
        task_id='list_jpeg_on_server',
        python_callable=list_files,
        op_kwargs={'source_location': '/usr/local/airflow/server'},
        dag=dag)
"""
"""
t_check_unanon_data_available = PythonOperator(
        task_id='sense_unanon_data_syno',
        python_callable=diff_directories,
        op_kwargs={'source_location': '/usr/local/airflow/syno/raw_data',
                   'target_location': '/usr/local/airflow/syno/anon_data',
                   'extension': '.mp4',
                   'ignore_location' : ['#recycle/']},
        dag=dag)
"""


# Set up DAG architecture
t_move_new_data_to_syno.set_upstream([t_check_new_data_available])

#t_check_unanon_data_available.set_upstream([t_move_new_data_to_syno])

op_kwargs={'source_location': '/usr/local/airflow/syno/raw_data',
           'target_location': '/usr/local/airflow/syno/anon_data',
           'extension': '.mp4',
           'ignore_location' : ['#recycle/']}

unanon_file_list = diff_directories(**op_kwargs)

for i, unanon_file in enumerate(unanon_file_list):
    t_list_unanon_files = PythonOperator(
        task_id='list_unanon_files_{:d}'.format(i),
        python_callable=list_unanon_files,
        provide_context=True,
        op_kwargs={'source_location': '/usr/local/airflow/syno/raw_data',
                   'target_location': '/usr/local/airflow/server',
                   'n_frame_threshold': 30*60*60*5,
                   'unanon_file': unanon_file},
        dag=dag)

    t_list_unanon_files.set_upstream([t_move_new_data_to_syno])

    t_split_to_frames_write_to_server = BashOperator(
        task_id='split_video_to_frames_write_to_server_{:d}'.format(i),
        bash_command=split_video_to_frames_write_to_server.format(i,i),
        params={'format': 'png',
                'width': '1280',
                'height': '720',
                'num_id': '{:d}'.format(i)},
        dag=dag)

    t_split_to_frames_write_to_server.set_upstream([t_list_unanon_files])

    """

    t_face_detect = BashOperator( # TODO: Add queue assignment
        task_id='tensorflow_face_detect',
        bash_command=tensorflow_face_detect,
        params={'gpu': 0,
                'cfg': '/usr/local/airflow/faster_rcnn/experiments/cfgs/faster_rcnn_end2end_wider.yml',
                'network': 'VGGnet_featconcat_test',
                'weights': '/usr/local/airflow/faster_rcnn/weights/VGGnet_fast_rcnn_iter_210000.ckpt',
                'num_classes': 2}, 
        dag=dag)

    t_face_blur = BashOperator(
        task_id='apply_face_blur',
        bash_command=apply_face_blur,
        params={'threshold': 0.5,
                'class_id': 1,
                'kernel': 9,
                'sigma': 8,
                'compression': 0}, 
        dag=dag)

    t_create_anon_dir = PythonOperator(
        task_id='create_anon_dir',
        python_callable=create_anon_dir,
        provide_context=True,
        dag=dag)


    t_combine_to_video_write_to_syno = BashOperator(
        task_id='combine_frames_to_video_write_to_syno',
        bash_command=combine_frames_to_video_write_to_syno,
        params={'format': 'png',
                'width': '1280',
                'height': '720',
                'fps': '30.0'},
        dag=dag)




t_face_detect.set_upstream([t_split_to_frames_write_to_server])

t_face_blur.set_upstream([t_face_detect])

t_create_anon_dir.set_upstream([t_face_blur])

t_combine_to_video_write_to_syno.set_upstream([t_create_anon_dir,t_list_unanon_files])

"""
